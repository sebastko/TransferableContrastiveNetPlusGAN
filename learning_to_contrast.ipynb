{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(class_embedding='att', dataroot='../datasets/data_resnet2', dataset='APY', disp_interval=500, evl_interval=1000, exp_idx='', gpu='3', image_embedding='res101', manualSeed=4683, matdataset=True, preprocessing=False, resume=None, save_interval=10000, standardization=False, validation=False, z_dim=100)\n",
      "('Random Seed: ', 4683)\n",
      "64 2048\n",
      "_AttributeNet(\n",
      "  (attribute): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=1024, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n",
      "_RelationNet(\n",
      "  (relation): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\u001b[31m The output dictionary is Result_origin_attr/GBU_APY/Rls0.01\u001b[0m\n",
      "torch.Size([20, 12])\n",
      "tensor([0.0582, 0.0181, 0.5576, 0.0000, 0.0130, 0.0133, 0.0874, 0.0310, 0.0000,\n",
      "        0.0000, 0.1781, 0.0432])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianghuajie/.local/lib/python2.7/site-packages/ipykernel_launcher.py:396: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/home/jianghuajie/.local/lib/python2.7/site-packages/ipykernel_launcher.py:397: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/jianghuajie/.local/lib/python2.7/site-packages/ipykernel_launcher.py:209: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-500; Loss: 0.187; ZSL: 0.000; H: 0.000; S: 0.000; U: 0.000;  \n",
      "Accuracy is 22.97%\n",
      "H 4.83%  S->T 25.80%  U->T 2.66%  \n",
      "Iter-1000; Loss: 0.140; ZSL: 22.975; H: 4.831; S: 25.798; U: 2.665;  \n",
      "Iter-1500; Loss: 0.104; ZSL: 22.975; H: 4.831; S: 25.798; U: 2.665;  \n",
      "Accuracy is 39.07%\n",
      "H 13.69%  S->T 55.20%  U->T 7.82%  \n",
      "Iter-2000; Loss: 0.096; ZSL: 39.070; H: 13.693; S: 55.204; U: 7.816;  \n",
      "Iter-2500; Loss: 0.084; ZSL: 39.070; H: 13.693; S: 55.204; U: 7.816;  \n",
      "Accuracy is 37.37%\n",
      "H 25.51%  S->T 59.69%  U->T 16.22%  \n",
      "Iter-3000; Loss: 0.059; ZSL: 37.373; H: 25.511; S: 59.690; U: 16.222;  \n",
      "Iter-3500; Loss: 0.054; ZSL: 37.373; H: 25.511; S: 59.690; U: 16.222;  \n",
      "Accuracy is 38.46%\n",
      "H 29.97%  S->T 62.66%  U->T 19.69%  \n",
      "Iter-4000; Loss: 0.040; ZSL: 38.459; H: 29.966; S: 62.662; U: 19.691;  \n",
      "Iter-4500; Loss: 0.042; ZSL: 38.459; H: 29.966; S: 62.662; U: 19.691;  \n",
      "Accuracy is 38.76%\n",
      "H 32.22%  S->T 64.03%  U->T 21.52%  \n",
      "Iter-5000; Loss: 0.046; ZSL: 38.757; H: 32.218; S: 64.033; U: 21.524;  \n",
      "Iter-5500; Loss: 0.044; ZSL: 38.757; H: 32.218; S: 64.033; U: 21.524;  \n",
      "Accuracy is 38.82%\n",
      "H 32.86%  S->T 65.79%  U->T 21.90%  \n",
      "Iter-6000; Loss: 0.043; ZSL: 38.824; H: 32.859; S: 65.788; U: 21.898;  \n",
      "Iter-6500; Loss: 0.046; ZSL: 38.824; H: 32.859; S: 65.788; U: 21.898;  \n",
      "Accuracy is 37.52%\n",
      "H 32.79%  S->T 67.90%  U->T 21.62%  \n",
      "Iter-7000; Loss: 0.031; ZSL: 37.519; H: 32.792; S: 67.898; U: 21.616;  \n",
      "Iter-7500; Loss: 0.033; ZSL: 37.519; H: 32.792; S: 67.898; U: 21.616;  \n",
      "Accuracy is 36.16%\n",
      "H 32.21%  S->T 70.32%  U->T 20.89%  \n",
      "Iter-8000; Loss: 0.030; ZSL: 36.159; H: 32.212; S: 70.319; U: 20.891;  \n",
      "Iter-8500; Loss: 0.033; ZSL: 36.159; H: 32.212; S: 70.319; U: 20.891;  \n",
      "Accuracy is 35.09%\n",
      "H 31.90%  S->T 70.93%  U->T 20.58%  \n",
      "Iter-9000; Loss: 0.054; ZSL: 35.094; H: 31.902; S: 70.933; U: 20.578;  \n",
      "Iter-9500; Loss: 0.020; ZSL: 35.094; H: 31.902; S: 70.933; U: 20.578;  \n",
      "Accuracy is 34.32%\n",
      "H 31.17%  S->T 72.99%  U->T 19.82%  \n",
      "\u001b[31mSave model to Result_origin_attr/GBU_APY/Rls0.01/Iter_10000.tar\u001b[0m\n",
      "Iter-10000; Loss: 0.050; ZSL: 34.322; H: 31.173; S: 72.987; U: 19.819;  \n",
      "Iter-10500; Loss: 0.034; ZSL: 34.322; H: 31.173; S: 72.987; U: 19.819;  \n",
      "Accuracy is 33.78%\n",
      "H 31.38%  S->T 73.44%  U->T 19.95%  \n",
      "Iter-11000; Loss: 0.027; ZSL: 33.778; H: 31.376; S: 73.444; U: 19.949;  \n",
      "Iter-11500; Loss: 0.020; ZSL: 33.778; H: 31.376; S: 73.444; U: 19.949;  \n",
      "Accuracy is 33.84%\n",
      "H 31.10%  S->T 74.04%  U->T 19.68%  \n",
      "Iter-12000; Loss: 0.017; ZSL: 33.839; H: 31.098; S: 74.039; U: 19.683;  \n",
      "Iter-12500; Loss: 0.027; ZSL: 33.839; H: 31.098; S: 74.039; U: 19.683;  \n",
      "Accuracy is 33.54%\n",
      "H 30.93%  S->T 74.86%  U->T 19.49%  \n",
      "Iter-13000; Loss: 0.023; ZSL: 33.545; H: 30.930; S: 74.857; U: 19.492;  \n",
      "Iter-13500; Loss: 0.014; ZSL: 33.545; H: 30.930; S: 74.857; U: 19.492;  \n",
      "Accuracy is 33.46%\n",
      "H 30.46%  S->T 75.00%  U->T 19.11%  \n",
      "Iter-14000; Loss: 0.012; ZSL: 33.464; H: 30.459; S: 74.998; U: 19.110;  \n",
      "Iter-14500; Loss: 0.031; ZSL: 33.464; H: 30.459; S: 74.998; U: 19.110;  \n",
      "Accuracy is 32.99%\n",
      "H 30.16%  S->T 76.06%  U->T 18.81%  \n",
      "Iter-15000; Loss: 0.022; ZSL: 32.993; H: 30.165; S: 76.057; U: 18.813;  \n",
      "Iter-15500; Loss: 0.018; ZSL: 32.993; H: 30.165; S: 76.057; U: 18.813;  \n",
      "Accuracy is 33.45%\n",
      "H 29.98%  S->T 75.18%  U->T 18.72%  \n",
      "Iter-16000; Loss: 0.014; ZSL: 33.447; H: 29.981; S: 75.183; U: 18.724;  \n",
      "Iter-16500; Loss: 0.030; ZSL: 33.447; H: 29.981; S: 75.183; U: 18.724;  \n",
      "Accuracy is 32.95%\n",
      "H 29.11%  S->T 76.18%  U->T 17.99%  \n",
      "Iter-17000; Loss: 0.018; ZSL: 32.951; H: 29.106; S: 76.176; U: 17.990;  \n",
      "Iter-17500; Loss: 0.015; ZSL: 32.951; H: 29.106; S: 76.176; U: 17.990;  \n",
      "Accuracy is 32.97%\n",
      "H 28.78%  S->T 77.71%  U->T 17.66%  \n",
      "Iter-18000; Loss: 0.012; ZSL: 32.970; H: 28.783; S: 77.712; U: 17.663;  \n",
      "Iter-18500; Loss: 0.018; ZSL: 32.970; H: 28.783; S: 77.712; U: 17.663;  \n",
      "Accuracy is 32.95%\n",
      "H 28.14%  S->T 79.98%  U->T 17.07%  \n",
      "Iter-19000; Loss: 0.015; ZSL: 32.951; H: 28.140; S: 79.979; U: 17.074;  \n",
      "Iter-19500; Loss: 0.026; ZSL: 32.951; H: 28.140; S: 79.979; U: 17.074;  \n",
      "Accuracy is 32.07%\n",
      "H 26.31%  S->T 81.09%  U->T 15.70%  \n",
      "\u001b[31mSave model to Result_origin_attr/GBU_APY/Rls0.01/Iter_20000.tar\u001b[0m\n",
      "Iter-20000; Loss: 0.011; ZSL: 32.066; H: 26.310; S: 81.085; U: 15.702;  \n",
      "Iter-20500; Loss: 0.010; ZSL: 32.066; H: 26.310; S: 81.085; U: 15.702;  \n",
      "Accuracy is 32.09%\n",
      "H 25.29%  S->T 81.73%  U->T 14.96%  \n",
      "Iter-21000; Loss: 0.014; ZSL: 32.091; H: 25.292; S: 81.731; U: 14.961;  \n",
      "Iter-21500; Loss: 0.015; ZSL: 32.091; H: 25.292; S: 81.731; U: 14.961;  \n",
      "Accuracy is 31.97%\n",
      "H 23.87%  S->T 81.95%  U->T 13.97%  \n",
      "Iter-22000; Loss: 0.006; ZSL: 31.975; H: 23.873; S: 81.952; U: 13.971;  \n",
      "Iter-22500; Loss: 0.011; ZSL: 31.975; H: 23.873; S: 81.952; U: 13.971;  \n",
      "Accuracy is 31.51%\n",
      "H 22.84%  S->T 82.45%  U->T 13.25%  \n",
      "Iter-23000; Loss: 0.019; ZSL: 31.507; H: 22.835; S: 82.448; U: 13.253;  \n",
      "Iter-23500; Loss: 0.008; ZSL: 31.507; H: 22.835; S: 82.448; U: 13.253;  \n",
      "Accuracy is 30.99%\n",
      "H 21.91%  S->T 83.11%  U->T 12.62%  \n",
      "Iter-24000; Loss: 0.012; ZSL: 30.993; H: 21.913; S: 83.113; U: 12.620;  \n",
      "Iter-24500; Loss: 0.018; ZSL: 30.993; H: 21.913; S: 83.113; U: 12.620;  \n",
      "Accuracy is 30.50%\n",
      "H 21.93%  S->T 83.60%  U->T 12.62%  \n",
      "Iter-25000; Loss: 0.005; ZSL: 30.497; H: 21.926; S: 83.602; U: 12.618;  \n",
      "Iter-25500; Loss: 0.009; ZSL: 30.497; H: 21.926; S: 83.602; U: 12.618;  \n",
      "Accuracy is 30.76%\n",
      "H 20.71%  S->T 83.69%  U->T 11.82%  \n",
      "Iter-26000; Loss: 0.008; ZSL: 30.759; H: 20.709; S: 83.695; U: 11.817;  \n",
      "Iter-26500; Loss: 0.005; ZSL: 30.759; H: 20.709; S: 83.695; U: 11.817;  \n",
      "Accuracy is 31.40%\n",
      "H 21.08%  S->T 84.34%  U->T 12.05%  \n",
      "Iter-27000; Loss: 0.007; ZSL: 31.400; H: 21.082; S: 84.335; U: 12.047;  \n",
      "Iter-27500; Loss: 0.012; ZSL: 31.400; H: 21.082; S: 84.335; U: 12.047;  \n",
      "Accuracy is 30.55%\n",
      "H 20.47%  S->T 84.52%  U->T 11.64%  \n",
      "Iter-28000; Loss: 0.013; ZSL: 30.548; H: 20.467; S: 84.517; U: 11.643;  \n",
      "Iter-28500; Loss: 0.009; ZSL: 30.548; H: 20.467; S: 84.517; U: 11.643;  \n",
      "Accuracy is 30.77%\n",
      "H 20.05%  S->T 84.79%  U->T 11.37%  \n",
      "Iter-29000; Loss: 0.009; ZSL: 30.767; H: 20.050; S: 84.790; U: 11.369;  \n",
      "Iter-29500; Loss: 0.006; ZSL: 30.767; H: 20.050; S: 84.790; U: 11.369;  \n",
      "Accuracy is 30.05%\n",
      "H 19.40%  S->T 85.06%  U->T 10.95%  \n",
      "\u001b[31mSave model to Result_origin_attr/GBU_APY/Rls0.01/Iter_30000.tar\u001b[0m\n",
      "Iter-30000; Loss: 0.015; ZSL: 30.052; H: 19.399; S: 85.063; U: 10.948;  \n",
      "Iter-30500; Loss: 0.009; ZSL: 30.052; H: 19.399; S: 85.063; U: 10.948;  \n",
      "Accuracy is 29.97%\n",
      "H 19.50%  S->T 85.39%  U->T 11.01%  \n",
      "Iter-31000; Loss: 0.013; ZSL: 29.975; H: 19.501; S: 85.389; U: 11.008;  \n",
      "Iter-31500; Loss: 0.010; ZSL: 29.975; H: 19.501; S: 85.389; U: 11.008;  \n",
      "Accuracy is 29.25%\n",
      "H 18.82%  S->T 85.25%  U->T 10.58%  \n",
      "Iter-32000; Loss: 0.009; ZSL: 29.249; H: 18.818; S: 85.250; U: 10.576;  \n",
      "Iter-32500; Loss: 0.005; ZSL: 29.249; H: 18.818; S: 85.250; U: 10.576;  \n",
      "Accuracy is 29.93%\n",
      "H 19.14%  S->T 85.16%  U->T 10.78%  \n",
      "Iter-33000; Loss: 0.010; ZSL: 29.929; H: 19.144; S: 85.165; U: 10.784;  \n",
      "Iter-33500; Loss: 0.007; ZSL: 29.929; H: 19.144; S: 85.165; U: 10.784;  \n",
      "Accuracy is 28.56%\n",
      "H 17.51%  S->T 85.18%  U->T 9.76%  \n",
      "Iter-34000; Loss: 0.013; ZSL: 28.561; H: 17.509; S: 85.175; U: 9.758;  \n",
      "Iter-34500; Loss: 0.009; ZSL: 28.561; H: 17.509; S: 85.175; U: 9.758;  \n",
      "Accuracy is 28.74%\n",
      "H 17.72%  S->T 85.10%  U->T 9.89%  \n",
      "Iter-35000; Loss: 0.006; ZSL: 28.744; H: 17.724; S: 85.104; U: 9.892;  \n",
      "Iter-35500; Loss: 0.003; ZSL: 28.744; H: 17.724; S: 85.104; U: 9.892;  \n",
      "Accuracy is 28.23%\n",
      "H 17.17%  S->T 85.34%  U->T 9.55%  \n",
      "Iter-36000; Loss: 0.005; ZSL: 28.229; H: 17.173; S: 85.339; U: 9.547;  \n",
      "Iter-36500; Loss: 0.005; ZSL: 28.229; H: 17.173; S: 85.339; U: 9.547;  \n",
      "Accuracy is 29.07%\n",
      "H 18.02%  S->T 85.39%  U->T 10.07%  \n",
      "Iter-37000; Loss: 0.005; ZSL: 29.067; H: 18.021; S: 85.389; U: 10.074;  \n",
      "Iter-37500; Loss: 0.006; ZSL: 29.067; H: 18.021; S: 85.389; U: 10.074;  \n",
      "Accuracy is 29.34%\n",
      "H 17.52%  S->T 85.10%  U->T 9.77%  \n",
      "Iter-38000; Loss: 0.005; ZSL: 29.341; H: 17.524; S: 85.103; U: 9.768;  \n",
      "Iter-38500; Loss: 0.005; ZSL: 29.341; H: 17.524; S: 85.103; U: 9.768;  \n",
      "Accuracy is 29.60%\n",
      "H 17.69%  S->T 85.16%  U->T 9.87%  \n",
      "Iter-39000; Loss: 0.012; ZSL: 29.598; H: 17.693; S: 85.164; U: 9.872;  \n",
      "Iter-39500; Loss: 0.005; ZSL: 29.598; H: 17.693; S: 85.164; U: 9.872;  \n",
      "Accuracy is 29.63%\n",
      "H 17.46%  S->T 85.26%  U->T 9.73%  \n",
      "\u001b[31mSave model to Result_origin_attr/GBU_APY/Rls0.01/Iter_40000.tar\u001b[0m\n",
      "Iter-40000; Loss: 0.015; ZSL: 29.629; H: 17.463; S: 85.257; U: 9.728;  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-40500; Loss: 0.004; ZSL: 29.629; H: 17.463; S: 85.257; U: 9.728;  \n",
      "Accuracy is 29.57%\n",
      "H 17.82%  S->T 84.57%  U->T 9.96%  \n",
      "Iter-41000; Loss: 0.005; ZSL: 29.572; H: 17.821; S: 84.573; U: 9.960;  \n",
      "Iter-41500; Loss: 0.005; ZSL: 29.572; H: 17.821; S: 84.573; U: 9.960;  \n",
      "Accuracy is 29.73%\n",
      "H 17.39%  S->T 85.25%  U->T 9.68%  \n",
      "Iter-42000; Loss: 0.005; ZSL: 29.727; H: 17.389; S: 85.253; U: 9.682;  \n",
      "Iter-42500; Loss: 0.007; ZSL: 29.727; H: 17.389; S: 85.253; U: 9.682;  \n",
      "Accuracy is 30.10%\n",
      "H 18.23%  S->T 84.79%  U->T 10.21%  \n",
      "Iter-43000; Loss: 0.003; ZSL: 30.097; H: 18.230; S: 84.795; U: 10.213;  \n",
      "Iter-43500; Loss: 0.003; ZSL: 30.097; H: 18.230; S: 84.795; U: 10.213;  \n",
      "Accuracy is 29.86%\n",
      "H 18.20%  S->T 84.47%  U->T 10.20%  \n",
      "Iter-44000; Loss: 0.004; ZSL: 29.864; H: 18.202; S: 84.475; U: 10.200;  \n",
      "Iter-44500; Loss: 0.006; ZSL: 29.864; H: 18.202; S: 84.475; U: 10.200;  \n",
      "Accuracy is 30.44%\n",
      "H 19.05%  S->T 84.69%  U->T 10.73%  \n",
      "Iter-45000; Loss: 0.008; ZSL: 30.440; H: 19.049; S: 84.690; U: 10.731;  \n",
      "Iter-45500; Loss: 0.004; ZSL: 30.440; H: 19.049; S: 84.690; U: 10.731;  \n",
      "Accuracy is 29.61%\n",
      "H 18.07%  S->T 84.99%  U->T 10.11%  \n",
      "Iter-46000; Loss: 0.003; ZSL: 29.613; H: 18.074; S: 84.986; U: 10.112;  \n",
      "Iter-46500; Loss: 0.003; ZSL: 29.613; H: 18.074; S: 84.986; U: 10.112;  \n",
      "Accuracy is 30.37%\n",
      "H 17.50%  S->T 84.61%  U->T 9.76%  \n",
      "Iter-47000; Loss: 0.005; ZSL: 30.370; H: 17.499; S: 84.606; U: 9.758;  \n",
      "Iter-47500; Loss: 0.003; ZSL: 30.370; H: 17.499; S: 84.606; U: 9.758;  \n",
      "Accuracy is 30.35%\n",
      "H 18.40%  S->T 84.66%  U->T 10.32%  \n",
      "Iter-48000; Loss: 0.008; ZSL: 30.349; H: 18.401; S: 84.663; U: 10.322;  \n",
      "Iter-48500; Loss: 0.011; ZSL: 30.349; H: 18.401; S: 84.663; U: 10.322;  \n",
      "Accuracy is 29.70%\n",
      "H 17.30%  S->T 83.55%  U->T 9.65%  \n",
      "Iter-49000; Loss: 0.003; ZSL: 29.700; H: 17.297; S: 83.550; U: 9.647;  \n",
      "Iter-49500; Loss: 0.003; ZSL: 29.700; H: 17.297; S: 83.550; U: 9.647;  \n",
      "Accuracy is 30.11%\n",
      "H 16.89%  S->T 83.54%  U->T 9.39%  \n",
      "\u001b[31mSave model to Result_origin_attr/GBU_APY/Rls0.01/Iter_50000.tar\u001b[0m\n",
      "Iter-50000; Loss: 0.004; ZSL: 30.109; H: 16.889; S: 83.537; U: 9.394;  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "\n",
    "from termcolor import cprint\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from dataset_loader import FeatDataLayer, DATA_LOADER\n",
    "from models import _AttributeNet, _RelationNet, _param\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='APY', help='FLO')\n",
    "parser.add_argument('--dataroot', default='../datasets/data_resnet2',\n",
    "                    help='path to dataset')\n",
    "parser.add_argument('--matdataset', default=True, help='Data in matlab format')\n",
    "parser.add_argument('--image_embedding', default='res101')\n",
    "parser.add_argument('--class_embedding', default='att')\n",
    "parser.add_argument('--preprocessing', action='store_true', default=False,\n",
    "                    help='enbale MinMaxScaler on visual features')\n",
    "parser.add_argument('--standardization', action='store_true', default=False)\n",
    "parser.add_argument('--validation', action='store_true', default=False, help='enable cross validation mode')\n",
    "\n",
    "parser.add_argument('--gpu', default='3', type=str, help='index of GPU to use')\n",
    "parser.add_argument('--exp_idx', default='', type=str, help='exp idx')\n",
    "parser.add_argument('--manualSeed', type=int, default=4683, help='manual seed')\n",
    "parser.add_argument('--resume',  type=str, help='the model to resume')\n",
    "\n",
    "parser.add_argument('--z_dim',  type=int, default=100, help='dimension of the random vector z')\n",
    "parser.add_argument('--disp_interval', type=int, default=500)\n",
    "parser.add_argument('--save_interval', type=int, default=10000)\n",
    "parser.add_argument('--evl_interval',  type=int, default=1000)\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "print(opt)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu\n",
    "\n",
    "\"\"\" hyper-parameter  \"\"\"\n",
    "opt.REG_W_LAMBDA = 0.01\n",
    "\n",
    "\n",
    "opt.lr = 0.00001\n",
    "opt.batch_size = 32  # 512\n",
    "\n",
    "\"\"\" hyper-parameter for testing\"\"\"\n",
    "\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "\n",
    "\n",
    "def train():\n",
    "    param = _param()\n",
    "    dataset = DATA_LOADER(opt)\n",
    "    param.X_dim = dataset.feature_dim\n",
    "\n",
    "    data_layer = FeatDataLayer(dataset.train_label.numpy(), dataset.train_feature.numpy(), opt)\n",
    "    result = Result()\n",
    "    result_gzsl = Result()\n",
    "    \n",
    "    print dataset.att_dim, dataset.feature_dim\n",
    "    APnet = _AttributeNet(dataset.att_dim, dataset.feature_dim).cuda()\n",
    "    APnet.apply(weights_init)\n",
    "    print(APnet)\n",
    "    \n",
    "    Rnet = _RelationNet(dataset.feature_dim).cuda()\n",
    "    Rnet.apply(weights_init)\n",
    "    print(Rnet)\n",
    "    \n",
    "    exp_info = 'GBU_{}'.format(opt.dataset)\n",
    "    exp_params = 'Rls{}'.format(opt.REG_W_LAMBDA)\n",
    "\n",
    "    out_dir  = 'Result/{:s}'.format(exp_info)\n",
    "    out_subdir = 'Result/{:s}/{:s}'.format(exp_info, exp_params)\n",
    "    if not os.path.exists('Result'):\n",
    "        os.mkdir('Result')\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "    if not os.path.exists(out_subdir):\n",
    "        os.mkdir(out_subdir)\n",
    "\n",
    "    cprint(\" The output dictionary is {}\".format(out_subdir), 'red')\n",
    "    log_dir = out_subdir + '/log_{:s}_{}.txt'.format(exp_info, opt.exp_idx)\n",
    "    with open(log_dir, 'w') as f:\n",
    "        f.write('Training Start:')\n",
    "        f.write(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime()) + '\\n')\n",
    "\n",
    "    start_step = 0\n",
    "\n",
    "    if opt.resume:\n",
    "        if os.path.isfile(opt.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(opt.resume))\n",
    "            checkpoint = torch.load(opt.resume)\n",
    "            APnet.load_state_dict(checkpoint['state_dict_AP'])\n",
    "            start_step = checkpoint['it']\n",
    "            print(checkpoint['log'])\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(opt.resume))\n",
    "\n",
    "    optimizerAP = optim.Adam(APnet.parameters(), lr=opt.lr, betas=(0.5, 0.9))\n",
    "    optimizerR = optim.Adam(Rnet.parameters(), lr=opt.lr, betas=(0.5, 0.9))\n",
    "    nets = [APnet, Rnet]\n",
    "    \n",
    "    acc = 0\n",
    "    H = 0\n",
    "    S = 0\n",
    "    U = 0\n",
    "    \n",
    "    all_class = torch.cat([dataset.seenclasses, dataset.unseenclasses],0)\n",
    "    print dataset.sim.shape\n",
    "    print dataset.sim[1]\n",
    "    \n",
    "    for it in range(start_step, 50000+1):      \n",
    "        blobs = data_layer.forward()\n",
    "        batch_feats = blobs['data']             # image data\n",
    "        batch_labels = blobs['labels'].astype(int)  # class labels\n",
    "        \n",
    "        support_attr = dataset.attribute[all_class].cuda()\n",
    "        class_num = all_class.shape[0]\n",
    "        \n",
    "        batch_images = torch.from_numpy(batch_feats).cuda()\n",
    "\n",
    "        batch_ext = batch_images.unsqueeze(0).repeat(class_num,1,1)\n",
    "        batch_ext = torch.transpose(batch_ext,0,1)\n",
    "                        \n",
    "        # forward\n",
    "        semantic_proto = APnet(support_attr)\n",
    "        semantic_proto_ext = semantic_proto.unsqueeze(0).repeat(opt.batch_size,1,1)\n",
    "        # relation_pairs = torch.cat([semantic_proto_ext, batch_ext],2).view(-1,dataset.feature_dim + dataset.feature_dim)\n",
    "        relation_pairs = semantic_proto_ext * batch_ext\n",
    "        relations = Rnet(relation_pairs).view(-1,class_num)\n",
    "        \n",
    "        # label transform\n",
    "        support_labels = dataset.train_class.numpy()\n",
    "        re_batch_labels = np.zeros_like(batch_labels)\n",
    "        train_class_num = support_labels.shape[0]\n",
    "        for i in range(train_class_num):\n",
    "            re_batch_labels[batch_labels==support_labels[i]] = i\n",
    "        re_batch_labels = torch.LongTensor(re_batch_labels)\n",
    "        \n",
    "        bce = nn.BCELoss().cuda()\n",
    "        mse = nn.MSELoss().cuda()\n",
    "        one_hot_labels = torch.zeros(opt.batch_size, train_class_num).scatter_(1, re_batch_labels.view(-1,1), 1).cuda()\n",
    "        sim_labels = dataset.sim[re_batch_labels].cuda()\n",
    "        \n",
    "        relation_train = relations[:, :train_class_num]\n",
    "        relation_test = relations[:, train_class_num:]\n",
    "        \n",
    "        loss1 = bce(relation_train,one_hot_labels)\n",
    "        loss2 = bce(relation_test,sim_labels)\n",
    "        loss = loss1 + opt.REG_W_LAMBDA *loss2\n",
    "        \n",
    "        reset_grad(nets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizerAP.step()\n",
    "        optimizerR.step()\n",
    "\n",
    "\n",
    "        if it % opt.evl_interval == 0 and it >= 100:\n",
    "            APnet.eval()\n",
    "            Rnet.eval()\n",
    "            acc = eval_test(it, APnet, Rnet, dataset, param, result)\n",
    "            if result.save_model:\n",
    "                files2remove = glob.glob(out_subdir + '/Best_model_ZSL_*')\n",
    "                for _i in files2remove:\n",
    "                    os.remove(_i)\n",
    "                # best_acc = result.acc_list[-1]\n",
    "                save_model(it, APnet, Rnet, opt.manualSeed, log_text,\n",
    "                           out_subdir + '/Best_model_ZSL_Acc_{:.2f}.tar'.format(result.acc_list[-1]))\n",
    "\n",
    "            H, S, U = eval_test_gzsl(it, APnet, Rnet, dataset, param, result_gzsl)\n",
    "            if result_gzsl.save_model:\n",
    "                files2remove = glob.glob(out_subdir + '/Best_model_GZSL_*')\n",
    "                for _i in files2remove:\n",
    "                    os.remove(_i)\n",
    "                # best_acc_gzsl = result.acc_list[-1]\n",
    "                save_model(it, APnet, Rnet, opt.manualSeed, log_text,\n",
    "                           out_subdir + '/Best_model_GZSL_H_{:.2f}_S_{:.2f}_U_{:.2f}.tar'.format(result_gzsl.best_acc,\n",
    "                                                                                                 result_gzsl.best_acc_S_T,\n",
    "                                                                                                 result_gzsl.best_acc_U_T))\n",
    "\n",
    "            APnet.train()\n",
    "            Rnet.train()\n",
    "\n",
    "        if it % opt.save_interval == 0 and it:\n",
    "            save_model(it, APnet, Rnet, opt.manualSeed, log_text,\n",
    "                       out_subdir + '/Iter_{:d}.tar'.format(it))\n",
    "            cprint('Save model to ' + out_subdir + '/Iter_{:d}.tar'.format(it), 'red')\n",
    "        \n",
    "        if it % opt.disp_interval == 0 and it:\n",
    "\n",
    "            log_text = 'Iter-{}; Loss: {:.3f}; ZSL: {:.3f}; H: {:.3f}; S: {:.3f}; U: {:.3f};  '\\\n",
    "                        .format(it, loss.data[0], acc, H, S, U)\n",
    "            print(log_text)\n",
    "            with open(log_dir, 'a') as f:\n",
    "                f.write(log_text+'\\n')\n",
    "\n",
    "def save_model(it, APnet, Rnet, random_seed, log, fout):\n",
    "    torch.save({\n",
    "        'it': it + 1,\n",
    "        'state_dict_AP': APnet.state_dict(),\n",
    "        'state_dict_R': Rnet.state_dict(),\n",
    "        'random_seed': random_seed,\n",
    "        'log': log,\n",
    "    }, fout)\n",
    "\n",
    "\n",
    "def eval_test(it, APnet, Rnet, dataset, param, result):\n",
    "    support_attr = torch.from_numpy(dataset.test_att).cuda()\n",
    "    class_num = support_attr.shape[0]\n",
    "    \n",
    "    # label transform\n",
    "    support_labels = np.array(dataset.unseenclasses)\n",
    "    test_labels = np.array(dataset.test_unseen_label)\n",
    "    \n",
    "    test_images = dataset.test_unseen_feature.cuda()\n",
    "    preds = np.zeros_like(test_labels)\n",
    "    \n",
    "    start = 0 \n",
    "    while start < test_labels.shape[0]:\n",
    "        if start + opt.batch_size > test_labels.shape[0]:\n",
    "            end = test_labels.shape[0]\n",
    "        else:\n",
    "            end = start + opt.batch_size\n",
    "        \n",
    "        batch_images = test_images[start:end]\n",
    "        \n",
    "        batch_ext = batch_images.unsqueeze(0).repeat(class_num,1,1)\n",
    "        batch_ext = torch.transpose(batch_ext,0,1)\n",
    "\n",
    "        # forward\n",
    "        semantic_proto = APnet(support_attr)\n",
    "        semantic_proto_ext = semantic_proto.unsqueeze(0).repeat(end-start,1,1)\n",
    "        # relation_pairs = torch.cat([semantic_proto_ext, batch_ext],2).view(-1,dataset.feature_dim + dataset.feature_dim)\n",
    "        relation_pairs = semantic_proto_ext * batch_ext\n",
    "        relations = Rnet(relation_pairs).view(-1,class_num)\n",
    "        prob = relations.data.cpu().numpy()\n",
    "        pred = np.argmax(prob,1)\n",
    "        preds[start:end]=pred\n",
    "        \n",
    "        start = end\n",
    "\n",
    "    # produce MCA\n",
    "    grounds = test_labels\n",
    "    predicts = support_labels[preds]\n",
    "\n",
    "    acc = np.zeros(class_num)\n",
    "    for i in range(class_num):\n",
    "        index = grounds == support_labels[i]\n",
    "        acc[i] = (predicts[index] == support_labels[i]).mean()\n",
    "    acc = acc.mean() * 100\n",
    "\n",
    "    result.acc_list += [acc]\n",
    "    result.iter_list += [it]\n",
    "    result.save_model = False\n",
    "    if acc > result.best_acc:\n",
    "        result.best_acc = acc\n",
    "        result.best_iter = it\n",
    "        result.save_model = True\n",
    "    print(\"Accuracy is {:.2f}%\".format(acc))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def eval_test_gzsl(it, APnet, Rnet, dataset, param, result):\n",
    "    support_attr = dataset.attribute.cuda()\n",
    "    class_num = support_attr.shape[0]\n",
    "    \n",
    "    # label transform\n",
    "    support_labels = np.array(dataset.allclasses)\n",
    "    \n",
    "    \"\"\"  S -> T\n",
    "    \"\"\"\n",
    "    test_labels = np.array(dataset.test_seen_label)\n",
    "    test_images = dataset.test_seen_feature.cuda()\n",
    "    preds = np.zeros_like(test_labels)\n",
    "    \n",
    "    start = 0 \n",
    "    while start < test_labels.shape[0]:\n",
    "        if start + opt.batch_size > test_labels.shape[0]:\n",
    "            end = test_labels.shape[0]\n",
    "        else:\n",
    "            end = start + opt.batch_size\n",
    "        \n",
    "        batch_images = test_images[start:end]\n",
    "        \n",
    "        batch_ext = batch_images.unsqueeze(0).repeat(class_num,1,1)\n",
    "        batch_ext = torch.transpose(batch_ext,0,1)\n",
    "\n",
    "        # forward\n",
    "        semantic_proto = APnet(support_attr)\n",
    "        semantic_proto_ext = semantic_proto.unsqueeze(0).repeat(end-start,1,1)\n",
    "        # relation_pairs = torch.cat([semantic_proto_ext, batch_ext],2).view(-1,dataset.feature_dim + dataset.feature_dim)\n",
    "        relation_pairs = semantic_proto_ext * batch_ext\n",
    "        relations = Rnet(relation_pairs).view(-1,class_num)\n",
    "        prob = relations.data.cpu().numpy()\n",
    "        pred = np.argmax(prob,1)\n",
    "        preds[start:end]=pred\n",
    "        \n",
    "        start = end\n",
    "\n",
    "    # produce MCA\n",
    "    label_T = test_labels\n",
    "    num_seen_classes = dataset.ntrain_class\n",
    "    seen_classes = np.array(dataset.seenclasses)\n",
    "    \n",
    "    acc = np.zeros(num_seen_classes)\n",
    "    for i in range(num_seen_classes):\n",
    "        acc[i] = (preds[label_T == seen_classes[i]] == seen_classes[i]).mean()\n",
    "    acc_S_T = acc.mean() * 100\n",
    "\n",
    "    \"\"\"  U -> T\n",
    "    \"\"\"\n",
    "    test_labels = np.array(dataset.test_unseen_label)\n",
    "    test_images = dataset.test_unseen_feature.cuda()\n",
    "    preds = np.zeros_like(test_labels)\n",
    "    \n",
    "    start = 0 \n",
    "    while start < test_labels.shape[0]:\n",
    "        if start + opt.batch_size > test_labels.shape[0]:\n",
    "            end = test_labels.shape[0]\n",
    "        else:\n",
    "            end = start + opt.batch_size\n",
    "        \n",
    "        batch_images = test_images[start:end]\n",
    "        \n",
    "        batch_ext = batch_images.unsqueeze(0).repeat(class_num,1,1)\n",
    "        batch_ext = torch.transpose(batch_ext,0,1)\n",
    "\n",
    "        # forward\n",
    "        semantic_proto = APnet(support_attr)\n",
    "        semantic_proto_ext = semantic_proto.unsqueeze(0).repeat(end-start,1,1)\n",
    "        # relation_pairs = torch.cat([semantic_proto_ext, batch_ext],2).view(-1,dataset.feature_dim + dataset.feature_dim)\n",
    "        relation_pairs = semantic_proto_ext * batch_ext\n",
    "        relations = Rnet(relation_pairs).view(-1,class_num)\n",
    "        prob = relations.data.cpu().numpy()\n",
    "        pred = np.argmax(prob,1)\n",
    "        preds[start:end]=pred\n",
    "        \n",
    "        start = end\n",
    "\n",
    "    # produce MCA\n",
    "    label_T = test_labels\n",
    "    num_unseen_classes = dataset.ntest_class\n",
    "    unseen_classes = np.array(dataset.unseenclasses)\n",
    "    \n",
    "    acc = np.zeros(num_unseen_classes)\n",
    "    for i in range(num_unseen_classes):\n",
    "        acc[i] = (preds[label_T == unseen_classes[i]] == unseen_classes[i]).mean()\n",
    "    acc_U_T = acc.mean() * 100\n",
    "    \n",
    "    acc = (2 * acc_S_T * acc_U_T) / (acc_S_T + acc_U_T)\n",
    "\n",
    "    result.acc_list += [acc]\n",
    "    result.iter_list += [it]\n",
    "    result.save_model = False\n",
    "    if acc > result.best_acc:\n",
    "        result.best_acc = acc\n",
    "        result.best_iter = it\n",
    "        result.best_acc_S_T = acc_S_T\n",
    "        result.best_acc_U_T = acc_U_T\n",
    "        result.save_model = True\n",
    "\n",
    "    print(\"H {:.2f}%  S->T {:.2f}%  U->T {:.2f}%  \".format(acc, acc_S_T, acc_U_T))\n",
    "    return acc, acc_S_T, acc_U_T\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.best_acc = 0.0\n",
    "        self.best_iter = 0.0\n",
    "        self.best_acc_S_T = 0.0\n",
    "        self.best_acc_U_T = 0.0\n",
    "        self.acc_list = []\n",
    "        self.iter_list = []\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if 'Linear' in classname:\n",
    "        init.xavier_normal(m.weight.data)\n",
    "        init.constant(m.bias, 0.0)\n",
    "\n",
    "\n",
    "def reset_grad(nets):\n",
    "    for net in nets:\n",
    "        net.zero_grad()\n",
    "\n",
    "\n",
    "def label2mat(labels, y_dim):\n",
    "    c = np.zeros([labels.shape[0], y_dim])\n",
    "    for idx, d in enumerate(labels):\n",
    "        c[idx, d] = 1\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
